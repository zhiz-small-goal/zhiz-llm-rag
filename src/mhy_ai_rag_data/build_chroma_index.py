#!/usr/bin/env python3
# build_chroma_index.py
# Build a local Chroma vector index from data_processed/text_units.jsonl (generated by extract_units.py).
#
# Default design choices (sane baseline):
# - Generator LLM: your local Qwen2.5-7B-Instruct (GGUF Q4_K_M) -> NOT used in this script
# - Embeddings: BAAI/bge-m3 (downloaded once, then cached; or point to a local model dir)
# - Vector store: Chroma PersistentClient (local on-disk)
#
# Usage:
#   1) Build index:
#      python build_chroma_index.py build --root . --units data_processed/text_units.jsonl --db chroma_db --collection rag_chunks
#
#   2) Quick query sanity-check (optional):
#      python build_chroma_index.py query --db chroma_db --collection rag_chunks --q "存档导入怎么做" --k 5
#
# Dependencies (install matrix):
#   - Stage-1 (默认): `pip install -e .` 仅需运行 inventory/units/validate/plan，不依赖 Chroma/Embedding。
#   - Stage-2 (可选): `pip install -e .[embed]` 安装向量库与 embedding 相关依赖。
#
# 重要：本模块同时承载「chunking/单位过滤」的共享逻辑。
# 为了让 Stage-1 也能 import 这些共享逻辑，本文件 **不得在 import-time** 强依赖 chromadb/sentence-transformers。
# 因此：
#   - chromadb / sentence-transformers 仅在 build/query 路径中按需导入。

from __future__ import annotations

import argparse
import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple


def _require_chromadb():
    """Import chromadb only when needed.

    Rationale: Stage-1 workflows (inventory/units/validate/plan) must be able to import
    this module for shared chunking logic without installing Stage-2 heavy deps.
    """

    try:
        import chromadb

        return chromadb
    except Exception as e:  # pragma: no cover
        raise ImportError(
            "chromadb not installed. Install Stage-2 deps via: pip install -e .[embed]"
        ) from e


def _require_sentence_transformers():
    """Import sentence-transformers only when needed."""

    try:
        from sentence_transformers import SentenceTransformer

        return SentenceTransformer
    except Exception as e:  # pragma: no cover
        raise ImportError(
            "sentence-transformers not installed. Install Stage-2 deps via: pip install -e .[embed]"
        ) from e


# ---------------------------
# Chunking
# ---------------------------

@dataclass
class ChunkConf:
    max_chars: int = 1200
    overlap_chars: int = 120
    min_chars: int = 200


def parse_note_kv(note: str) -> Dict[str, str]:
    """
    Parse note like "access=public;use=allow;pii=no" into metadata fields.
    Any malformed pieces are ignored.
    """
    out: Dict[str, str] = {}
    if not note:
        return out
    parts = [p.strip() for p in note.split(";") if p.strip()]
    for p in parts:
        if "=" not in p:
            continue
        k, v = p.split("=", 1)
        k = k.strip()
        v = v.strip()
        if k:
            out[k] = v
    return out


def normalize_text(s: str) -> str:
    # Normalize line endings, trim trailing spaces, collapse excessive blank lines.
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = re.sub(r"[ \t]+\n", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s).strip()
    return s


def split_paragraphs(text: str) -> List[str]:
    text = normalize_text(text)
    if not text:
        return []
    paras = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
    return paras


def pack_paragraphs_to_chunks(paras: List[str], conf: ChunkConf) -> List[str]:
    """
    Greedy pack paragraphs into chunks, then add overlap by tail-carryover.
    Character-based chunking is robust for Chinese corpora.
    """
    chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0

    def flush():
        nonlocal cur, cur_len
        if not cur:
            return
        chunk = "\n\n".join(cur).strip()
        if chunk:
            chunks.append(chunk)
        cur = []
        cur_len = 0

    for p in paras:
        p_len = len(p)
        if p_len > conf.max_chars:
            # Paragraph too long: hard-split it.
            start = 0
            while start < p_len:
                end = min(start + conf.max_chars, p_len)
                piece = p[start:end].strip()
                if piece:
                    flush()
                    chunks.append(piece)
                start = end
            continue

        if cur_len + p_len + (2 if cur else 0) <= conf.max_chars:
            cur.append(p)
            cur_len += p_len + (2 if cur_len else 0)
        else:
            flush()
            cur.append(p)
            cur_len = p_len

    flush()

    # Add overlap (tail of previous chunk) to next chunk
    if conf.overlap_chars > 0 and len(chunks) > 1:
        out: List[str] = []
        prev_tail = ""
        for i, c in enumerate(chunks):
            if i == 0:
                out.append(c)
                prev_tail = c[-conf.overlap_chars:]
                continue
            merged_text = (prev_tail + "\n" + c).strip()
            out.append(merged_text)
            prev_tail = c[-conf.overlap_chars:]
        chunks = out

    # Merge very small chunks into previous where possible
    merged: List[str] = []
    for c in chunks:
        if len(c) < conf.min_chars and merged:
            merged[-1] = (merged[-1] + "\n\n" + c).strip()
        else:
            merged.append(c)
    return merged


# ---------------------------
# IO
# ---------------------------

def iter_units(units_path: Path) -> Iterable[Dict[str, Any]]:
    with units_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            yield json.loads(line)


def should_index_unit(unit: Dict[str, Any], include_media_stub: bool) -> bool:
    st = str(unit.get("source_type", "")).lower()
    if st in {"md", "txt", "html", "code", "other"}:
        return True
    if include_media_stub and st in {"image", "video"}:
        return True
    return False


def build_chunks_from_unit(unit: Dict[str, Any], conf: ChunkConf) -> Tuple[List[str], Dict[str, Any]]:
    """
    Returns (chunk_texts, base_metadata)
    """
    text = str(unit.get("text", "") or "")
    st = str(unit.get("source_type", "")).lower()

    paras = split_paragraphs(text)
    chunks = pack_paragraphs_to_chunks(paras, conf)

    base_md: Dict[str, Any] = {
        "doc_id": unit.get("doc_id"),
        "source_uri": unit.get("source_uri"),
        "source_type": st,
        "locator": unit.get("locator"),
        "updated_at": unit.get("updated_at"),
        "content_sha256": unit.get("content_sha256"),
    }

    note = str(unit.get("note", "") or "")
    base_md.update(parse_note_kv(note))
    base_md["note_raw"] = note

    if st == "md":
        # Chroma metadata values must be scalar (str/int/float/bool/None).
        # Store refs as JSON strings + counts.
        import json as _json
        asset_refs = unit.get("asset_refs", []) or []
        doc_refs = unit.get("doc_refs", []) or []
        base_md["asset_refs_n"] = int(len(asset_refs))
        base_md["doc_refs_n"] = int(len(doc_refs))
        base_md["asset_refs_json"] = _json.dumps(asset_refs, ensure_ascii=False)
        base_md["doc_refs_json"] = _json.dumps(doc_refs, ensure_ascii=False)

    return chunks, base_md


# ---------------------------
# Chroma
# ---------------------------

def get_chroma_collection(db_path: Path, name: str):
    # PersistentClient: data is stored on disk and loaded automatically.
    chromadb = _require_chromadb()
    client = chromadb.PersistentClient(path=str(db_path))
    # No embedding_function here because we pass embeddings explicitly at upsert time.
    return client.get_or_create_collection(name=name, metadata={"hnsw:space": "cosine"})


# ---------------------------
# Embeddings
# ---------------------------

def load_embedder(model_name_or_path: str, device: Optional[str]) -> Any:
    """
    model_name_or_path:
      - HF repo id like "BAAI/bge-m3" (downloads once; then cached)
      - or a local directory containing a SentenceTransformer model
    """
    SentenceTransformer = _require_sentence_transformers()
    if device:
        return SentenceTransformer(model_name_or_path, device=device)
    return SentenceTransformer(model_name_or_path)


def embed_texts(model: Any, texts: List[str], batch_size: int) -> List[List[float]]:
    # normalize_embeddings=True is generally preferred for cosine similarity search.
    emb = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True,
    )
    return [e.astype("float32").tolist() for e in emb]


# ---------------------------
# Build / Query
# ---------------------------

def cmd_build(args: argparse.Namespace) -> int:
    root = Path(args.root).resolve()
    units_path = (root / args.units).resolve()
    db_path = (root / args.db).resolve()
    db_path.mkdir(parents=True, exist_ok=True)

    if not units_path.exists():
        print(f"[FATAL] units not found: {units_path}")
        return 2

    conf = ChunkConf(max_chars=args.chunk_chars, overlap_chars=args.overlap_chars, min_chars=args.min_chunk_chars)

    try:
        collection = get_chroma_collection(db_path, args.collection)
    except ImportError as e:
        print("[FATAL] chromadb is required for build/query but is not installed.")
        print("        Install Stage-2 deps: pip install -e .[embed]")
        print(f"        Detail: {e}")
        return 2

    try:
        embedder = load_embedder(args.embed_model, args.device)
    except ImportError as e:
        print("[FATAL] sentence-transformers is required for build/query but is not installed.")
        print("        Install Stage-2 deps: pip install -e .[embed]")
        print(f"        Detail: {e}")
        return 2

    ids: List[str] = []
    docs: List[str] = []
    metas: List[Dict[str, Any]] = []

    total_units = 0
    units_indexed = 0
    units_skipped = 0
    total_chunks = 0
    type_breakdown: Dict[str, Dict[str, int]] = {}

    for unit in iter_units(units_path):
        total_units += 1
        st = str(unit.get("source_type", "") or "").lower()
        type_breakdown.setdefault(st, {"indexed": 0, "skipped": 0, "chunks": 0})

        if not should_index_unit(unit, args.include_media_stub):
            units_skipped += 1
            type_breakdown[st]["skipped"] += 1
            continue

        units_indexed += 1
        type_breakdown[st]["indexed"] += 1

        chunk_texts, base_md = build_chunks_from_unit(unit, conf)
        if not chunk_texts:
            continue

        doc_id = str(base_md.get("doc_id"))
        for idx, ct in enumerate(chunk_texts):
            chunk_id = f"{doc_id}:{idx}"
            ids.append(chunk_id)
            docs.append(ct)
            md = dict(base_md)
            md["chunk_index"] = idx
            md["chunk_chars"] = len(ct)
            metas.append(md)

        total_chunks += len(chunk_texts)
        type_breakdown[st]["chunks"] += len(chunk_texts)

        if len(ids) >= args.upsert_batch:
            embs = embed_texts(embedder, docs, batch_size=args.embed_batch)
            collection.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=embs)
            ids, docs, metas = [], [], []

    if ids:
        embs = embed_texts(embedder, docs, batch_size=args.embed_batch)
        collection.upsert(ids=ids, documents=docs, metadatas=metas, embeddings=embs)

    print("=== BUILD DONE ===")
    print(f"units_read={total_units}")
    print(f"units_indexed={units_indexed}")
    print(f"units_skipped={units_skipped}")
    print(f"chunks_indexed={total_chunks}")
    print(f"include_media_stub={args.include_media_stub}")
    print(f"chunk_conf=chunk_chars:{args.chunk_chars} overlap_chars:{args.overlap_chars} min_chunk_chars:{args.min_chunk_chars}")
    print(f"db_path={db_path}")
    print(f"collection={args.collection}")
    print(f"embed_model={args.embed_model} (cached after first download)")
    # compact breakdown for debugging / postmortem
    top = sorted(((k, v.get("chunks", 0)) for k, v in type_breakdown.items()), key=lambda x: x[1], reverse=True)[:8]
    print(f"type_breakdown.top_chunks={top}")

    # write DB build stamp (stable freshness basis for rag-status)
    try:
        from mhy_ai_rag_data.tools.write_db_build_stamp import write_db_build_stamp
        state_root = (root / "data_processed" / "index_state").resolve()
        stamp_out = write_db_build_stamp(
            root=root,
            db=db_path,
            collection=str(args.collection),
            state_root=state_root,
            plan_path=None,
            collection_count=int(collection.count()) if collection else None,
            writer="build_chroma_index",
        )
        print(f"[OK] wrote db_build_stamp: {stamp_out}")
    except Exception as e:  # noqa: BLE001
        print(f"[WARN] failed to write db_build_stamp.json: {type(e).__name__}: {e}")
    return 0


def cmd_query(args: argparse.Namespace) -> int:
    db_path = Path(args.db).resolve()
    try:
        collection = get_chroma_collection(db_path, args.collection)
    except ImportError as e:
        print("[FATAL] chromadb is required for query but is not installed.")
        print("        Install Stage-2 deps: pip install -e .[embed]")
        print(f"        Detail: {e}")
        return 2

    try:
        embedder = load_embedder(args.embed_model, args.device)
    except ImportError as e:
        print("[FATAL] sentence-transformers is required for query but is not installed.")
        print("        Install Stage-2 deps: pip install -e .[embed]")
        print(f"        Detail: {e}")
        return 2
    q_emb = embed_texts(embedder, [args.q], batch_size=1)[0]

    where = None
    if args.where:
        where = {}
        for kv in args.where.split(","):
            kv = kv.strip()
            if not kv or "=" not in kv:
                continue
            k, v = kv.split("=", 1)
            where[k.strip()] = v.strip()

    res = collection.query(
        query_embeddings=[q_emb],
        n_results=args.k,
        where=where,
        include=["documents", "metadatas", "distances"],
    )

    docs = (res.get("documents") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]
    dists = (res.get("distances") or [[]])[0]

    print("=== TOP RESULTS ===")
    for i, (doc, md, dist) in enumerate(zip(docs, metas, dists), 1):
        print(f"\n[{i}] distance={dist}")
        print(f"source_uri={md.get('source_uri')}")
        print(f"locator={md.get('locator')}")
        preview = (doc[:400] + "...") if isinstance(doc, str) and len(doc) > 400 else doc
        print(preview)

    return 0


def build_arg_parser() -> argparse.ArgumentParser:
    ap = argparse.ArgumentParser(description="Build/query a local Chroma index for RAG (chunks + embeddings).")
    sub = ap.add_subparsers(dest="cmd", required=True)

    b = sub.add_parser("build", help="Build (or rebuild) the Chroma index from text_units.jsonl")
    b.add_argument("--root", default=".", help="Project root directory")
    b.add_argument("--units", default="data_processed/text_units.jsonl", help="Units JSONL path (relative to root)")
    b.add_argument("--db", default="chroma_db", help="Chroma persistent db directory (relative to root)")
    b.add_argument("--collection", default="rag_chunks", help="Chroma collection name")

    b.add_argument("--embed-model", default="BAAI/bge-m3", help="Embedding model repo id or local dir")
    b.add_argument("--device", default=None, help='e.g. "cpu", "cuda", "cuda:0" (default: auto)')
    b.add_argument("--embed-batch", type=int, default=32, help="Embedding batch size")
    b.add_argument("--upsert-batch", type=int, default=256, help="Upsert batch size (chunks per write)")

    b.add_argument("--chunk-chars", type=int, default=1200, help="Max characters per chunk")
    b.add_argument("--overlap-chars", type=int, default=120, help="Overlap characters between neighboring chunks")
    b.add_argument("--min-chunk-chars", type=int, default=200, help="Merge chunks smaller than this into previous")
    b.add_argument("--include-media-stub", action="store_true", help="Also index image/video stub texts")

    q = sub.add_parser("query", help="Quick sanity query against an existing Chroma index")
    q.add_argument("--db", default="chroma_db", help="Chroma persistent db directory")
    q.add_argument("--collection", default="rag_chunks", help="Chroma collection name")
    q.add_argument("--q", required=True, help="Query text")
    q.add_argument("--k", type=int, default=5, help="Top-k results")
    q.add_argument("--where", default=None, help='Metadata filter, e.g. "access=public,pii=no"')

    q.add_argument("--embed-model", default="BAAI/bge-m3", help="Embedding model repo id or local dir")
    q.add_argument("--device", default=None, help='e.g. "cpu", "cuda", "cuda:0" (default: auto)')

    return ap


def main() -> None:
    ap = build_arg_parser()
    args = ap.parse_args()

    if args.cmd == "build":
        raise SystemExit(cmd_build(args))
    if args.cmd == "query":
        raise SystemExit(cmd_query(args))
    raise SystemExit(2)


if __name__ == "__main__":
    main()
